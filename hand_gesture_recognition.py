# -*- coding: utf-8 -*-
"""Hand Gesture Recognition

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/155NfgKOfrVAv5FWq7YRo4OU6eYijjLfW
"""

import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

X=np.load("X.npy")
Y=np.load("Y.npy")

x_9 = X[:204]
x_0 = X[204:409]
x_7 = X[409:615]
x_6 = X[615:822]
x_1 = X[822:1028]
x_8 = X[1028:1236]
x_4 = X[1236:1443]
x_3 = X[1443:1649]
x_2 = X[1649:1855]
x_5 = X[1855:]
X = np.concatenate((x_0,x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9))

X.shape

Y.shape

X

Y

import matplotlib.pyplot as plt
plt.imshow(X[203])
print(Y[203])
plt.show()
plt.imshow(X[210])
print(Y[210])
plt.show()
plt.imshow(X[420])
print(Y[420])
plt.show()
plt.imshow(X[618])
print(Y[618])
plt.show()
plt.imshow(X[850])
print(Y[850])
plt.show()
plt.imshow(X[1030])
print(Y[1030])
plt.show()
plt.imshow(X[1238])
print(Y[1238])
plt.show()
plt.imshow(X[1450])
print(Y[1450])
plt.show()
plt.imshow(X[1660])
print(Y[1660])
plt.show()
plt.imshow(X[1888])
print(Y[1888])
plt.show()

print(X[0])

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)

X_train.shape

X_test.shape

for i in range(0,50):
    plt.imshow(X_test[i])
    plt.show()
    print(Y_test[i])

X_train = X_train.reshape(X_train.shape[0], 64*64)
X_test = X_test.reshape(X_test.shape[0], 64*64)

print(X_train)

print(X_test)

X_train = X_train.reshape(X_train.shape[0], 64*64)
X_test = X_test.reshape(X_test.shape[0], 64*64)

X_train.shape

X_test.shape

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
model = Sequential()
model.add(Dense(units = 200, activation = "relu", input_dim = 4096))
model.add(Dense(units = 100, activation = "relu"))
model.add(Dense(units = 50, activation = "relu"))
model.add(Dense(units = 25, activation = "relu"))
model.add(Dense(units = 10, activation = "softmax"))
optimizer = Adam(lr = 0.001, beta_1= 0.9, beta_2 = 0.999)
model.compile(optimizer = optimizer, loss = "binary_crossentropy", metrics = ["accuracy"])
model.fit(X_train, Y_train, epochs= 800, batch_size = 300, verbose = 1 )

X_test.shape

from sklearn.metrics import accuracy_score
Y_pred = model.predict_classes(X_test)
Y_test_classes = np.argmax(Y_test, axis = 1)
ac = accuracy_score(Y_test_classes, Y_pred)
print("Accuracy of the NN Model is : ", ac,"%")

Y_pred

Y_test_classes

import cv2
import math

cam = cv2.VideoCapture(0)

cv2.namedWindow("test")

img_counter = 0

while True:
    ret, frame = cam.read()
    
    if not ret:
        print("failed to grab frame")
        break

    cv2.rectangle(frame, (300,300), (100,100), (0,255,0),0)
    crop_img = frame[100:300, 100:300]
    cv2.imshow("test", frame)

    k = cv2.waitKey(1)
    if k%256 == 27:
        # ESC pressed
        print("Escape hit, closing...")
        break
    elif k%256 == 32:
        # SPACE pressed
        img_name = "opencv_frame_{}.png".format(img_counter)
        cv2.imwrite(img_name, crop_img)
        print("{} written!".format(img_name))
        img_counter += 1

cam.release()

cv2.destroyAllWindows()

image=cv2.imread("opencv_frame_0.png")
import matplotlib.pyplot as plt

plt.imshow(image)

gray=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

plt.imshow(gray)

resized=cv2.resize(gray, (64,64),interpolation=cv2.INTER_AREA)

plt.imshow(resized)

print(resized)

import tensorflow as tf

newimg=tf.keras.utils.normalize(resized, axis=1)

print(newimg)

newimg.shape



newimg=np.array(newimg).reshape(1,64,64)

newimg.shape

print(newimg)

newimg = newimg.reshape(newimg.shape[0], 64*64)

newimg.shape

Y_pred = model.predict_classes(newimg)

print("The output of the captured gesture is= ",Y_pred[0])



